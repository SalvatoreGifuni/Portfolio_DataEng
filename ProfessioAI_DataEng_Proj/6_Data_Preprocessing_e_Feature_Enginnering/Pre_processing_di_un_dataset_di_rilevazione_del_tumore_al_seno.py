# -*- coding: utf-8 -*-
"""Pre-processing di un Dataset di Rilevazione del Tumore al Seno.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VxdRaUYyRYAGnJVAqfkmRR0LSFlinoiH

#**Before starting...**

This notebook works if a custom module called 'my_transformer.py' and the dataset 'sample_dataset.csv' is used, which you can download from my github by [clicking here](https://github.com/SalvatoreGifuni/Pre-processing-di-un-Dataset-di-Rilevazione-del-Tumore-al-Seno.git).

Then you can upload them at the same time in the box just below 'Importing Modules'.

#**Pre-processing di un Dataset di Rilevazione del Tumore al Seno**

**Project Overview**

The project focuses on preprocessing a breast cancer detection dataset to ensure data quality and readiness for machine learning models. By creating a robust and automated preprocessing pipeline using Scikit-learn's tools, the aim is to transform raw data into a format that enhances model performance.

**Objectives and Benefits**

Automation and Scalability: Ensures consistent and replicable data handling, reducing preparation time.
Data Quality Optimization: Improves data quality through cleaning and handling of skewed variables.
Customized Techniques: Allows for advanced transformations like skewness analysis, PCA, and feature selection.
Pipeline Descriptions

*Pipeline for Target = 1*: Focuses on positive cancer detection cases, addressing missing values, symmetrization, one-hot encoding, and standardization.

*Pipeline for All Records: *Transforms all dataset records with missing value strategies, binning for numerical variables, ordinal encoding, and feature selection.

*Pipeline for Numerical Variables:* Applies missing value cleaning, PCA for dimensionality reduction, symmetrization, and normalization.

**Final Outcome**

A comprehensive preprocessing object will be created, ready for production use, offering a scalable solution for complex datasets and robust model training.

#**Importing Modules and Useful Functions**

##*Importing Modules*
"""

# Upload 'my_transformer.py' and 'sample_dataset.csv'
from google.colab import files
uploaded = files.upload()

# Checking the Existence of Files
!ls

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import my_transformer as myt
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.decomposition import PCA
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.preprocessing import PowerTransformer, OneHotEncoder, StandardScaler, OrdinalEncoder, KBinsDiscretizer, MinMaxScaler, FunctionTransformer
from sklearn.feature_selection import SelectKBest, chi2
from joblib import dump
from typing import Union, Optional

"""##*Constants*"""

LINE_SEP = '\n'+'-'*30+'\n'

"""#**EDA**

##*General considerations*
"""

# Importing the dataset
df = pd.read_csv("sample_dataset.csv")
print('Total Rows x Columns:', df.shape)

print(LINE_SEP)

# Show the first 20 rows of the dataset
df.head(20)

"""On initial examination, the dataset appears to comprise predominantly numeric variables with a wide range of domains, in addition to the presence of null values.

The variable designated as 'target' appears to contain only two values, '0' and '1', which could be classified as a categorical discrete variable.

These preliminary observations will be subjected to further scrutiny through an in-depth exploratory analysis of the data.
"""

# Show generic dataset informations
df.info()

"""It can be seen that the dataset consists of several non-zero values in all variables except the target variable.

We will further check the absence of non-zero values for the variable 'target' (this is observed by the fact that the number of non-zero values is equal to the number of observations).

All the characteristics are numeric, except for the 'area error' which is of the 'object' type.
"""

# Show column descriptions 0 to 10
df.iloc[:,:11].describe()

# Show column descriptions 11 to 20
df.iloc[:,11:21].describe()

# Show column descriptions 21 to 30, considering that the only categorical variable is not included in the ‘describe’.
df.iloc[:,21:31].describe()

# Show the categorical variable descriptions
print(df.describe(include=['object']))

print(LINE_SEP)

# Show the count of values for the categorical variable
print("Possible values of the area error variable.\n", df['area error'].value_counts())

"""The categorical variable is considered separately as '.describe()' only reads numeric variables.

The variable is categorical ordinal, with a high frequency of value 'A' and a very low frequency of variables 'B' and 'C'.

Check that the target column does not contain null values.
"""

# Double check.

# Return the names of columns with non-null values.
non_null_columns = df.columns[df.notna().all()]
print('The variables with non-null values are:', non_null_columns)

print(LINE_SEP)

# Verify the equality between the non-null values of the dataset excluding the 'target' variable and the non-null values of the entire dataset.
non_null_sum_excluding_target = df.iloc[:, :-1].isna().sum().sum()
non_null_sum_total = df.isna().sum().sum()
print("Equality between the sum of non-null values of the 'dataset - target' and the sum of non-null values of the dataset:",
      non_null_sum_excluding_target == non_null_sum_total)

"""It is confirmed that the variable 'target' does not contain null values.

The values of the 'target' column are checked to classify it as a categorical or numerical variable.
"""

# Verification of target variable values.
print('Values of target variable\n')
print(df['target'].value_counts())

print(LINE_SEP)

print(df['target'].unique())

"""Although the variable is numerical, it can be classified as categorical because it has only two values '0' and '1', i.e. a binary variable that divides the observations into two different categories: '1' represents positive cases, i.e. observations that belong to this category; '0' indicates that the observations do not belong to category '1'.

A slight discrepancy is observed between the values of 1 and 0. While the number of values equal to 1 is greater, the discrepancy is deemed to be negligible when considered in the context of the totality of observations.

The presence of duplicate observations is checked.
"""

# Check for duplicate observations
df.duplicated().value_counts()

"""No instances of replication were identified.

##*Examination of the symmetry of features and outliners*
"""

# The dataset is divided into two distinct categories: independent and target variables.
X = df.iloc[:,:-1]
y = df['target']

# Subsequently, the independent variables are subdivided into two further categories: numeric and categorical variables.
numerical = X.select_dtypes(exclude = ['object'])
numerical_columns = numerical.columns

# A symmetry analysis will be conducted on the numerical variables presented in the boxplot graphs and histograms.

# Boxplot graphs. Furthermore, boxplot graphs facilitate the identification of outliers.
skew_dic = myt.generate_symmetry_graphs(X, figsize=(24,24), y_max_input = 0.93, fontsize = 12,
                         x_max_input = 0.75, colors=('blue', 'green'), skew_threshold=1, return_skew_dic = True)

# Calculate the average skewness
average_skewness = sum(skew_dic.values()) / len(skew_dic)

# Print the average skewness
print(f'Average Skewness: {average_skewness:.2f}')
print(f'Min Skewness: {min(skew_dic.values())}')
print(f'Max Skewness: {max(skew_dic.values())}')

myt.generate_symmetry_graphs(X, figsize=(24,24), y_max_input = 0.93, fontsize = 12, graph = 'hist',
                         x_max_input = 0.65, colors=('blue', 'green'), skew_threshold=1, return_skew_dic = False)

"""The graphs demonstrate that no variable exhibits perfect symmetry, with a skewness value ranging from a minimum of 0.42 to a maximum of 3.99, with an average of approximately 1.60.

It is proposed that a suitable threshold for distinguishing between symmetrical and asymmetrical variables is a skewness value of 1.
"""

asymmetric_feature = [keys for keys, value in skew_dic.items() if value > 1]
symmetric_feature = [keys for keys, value in skew_dic.items() if value < 1]

# Show the name of asymmetric variables
print('The names of asymmetric variables are:\n', asymmetric_feature)

# Show the count of asymmetric variables
print('\nThe count of asymmetric features is:', len(asymmetric_feature))

print(LINE_SEP)

# Show the name of symmetric variables
print('The names of symmetric variables are:\n', symmetric_feature)

# Show the count of symmetric variables
print('\nThe count of asymmetric features is:', len(symmetric_feature))

# Show X (features)
X.head(5)

# Show y (target)
y.head(5)

"""#**Pipeline 1**"""

# Set the threshold as obtained from the EDA
skew_threshold = 1

# Create the imputer for cleaning missing numeric asymmetric variables
asymmetric_imputer_pipe1 = SimpleImputer(strategy='median')

# Create the imputer for cleaning missing numeric symmetric variables
symmetric_imputer_pipe1 = SimpleImputer()

# Create the imputer for cleaning missing categorical variables
categorical_imputer_pipe1 = SimpleImputer(strategy='most_frequent')

# Create the transformer to make asymmetrical numerical variables symmetrical
transformer_pipe1 = PowerTransformer()   #using Yeo-Johnson method

# Create the encoder to make non-numeric categorical variables numeric
onehot_encoder_pipe1 = OneHotEncoder(categories = [['C','B','A']], sparse_output = False,
                                     drop = 'first')   #Drop the first variable to reduce the dimensionality of the dataset
                                                       #and avoid ‘dummy variable trap’ (collinearity of variables)

# Create the numerical variable standardiser
standardiser_pipe1 = StandardScaler()

# Make pipeline for numeric asymmetric variables
pipe1_asymmetric = make_pipeline(asymmetric_imputer_pipe1, transformer_pipe1, standardiser_pipe1)

# Make pipeline for numeric symmetric variables
pipe1_symmetric = make_pipeline(symmetric_imputer_pipe1, standardiser_pipe1)

# Make pipeline for categorical variables
pipe1_categorical = make_pipeline(categorical_imputer_pipe1, onehot_encoder_pipe1)

# Create a column transformer to apply pipelines to specify variables in the dataset
ct_pipe1 = ColumnTransformer([
                              ('numerical_asymmetric', pipe1_asymmetric, [col for col in numerical_columns if abs(X[col].skew()) >= skew_threshold]),
                              ('numerical_symmetric', pipe1_symmetric, [col for col in numerical_columns if abs(X[col].skew()) < skew_threshold]),
                              ('categorical', pipe1_categorical, make_column_selector(dtype_include=['object', 'boolean', 'category']))
                              ])

# Make the column transformer a pipeline
final_pipe1 = Pipeline([
                        ('tranform_pipe1', ct_pipe1)
                        ])

"""##*Testing Pipeline 1*"""

# Applying Pipeline 1 on the dataset
X_pipe1 = final_pipe1.fit_transform(X)

# Show the new dataset
X_pipe1

"""It was observed that 31 features were returned, comprising the sum of the 29 numerical features and the 2 categorical features derived from the one-hot encoder.

The expected outcome of encoder would have been 3, reflecting the presence of 3 values for the categorical variable; however, the decision was taken to exclude the initial value.

#**Pipeline 2**
"""

# Create the imputer for cleaning missing numeric variables
# Because most numerical variables have outliers - using a method that is more robust in the presence of outliers.
numerical_imputer_pipe2 = SimpleImputer(strategy = 'median')

# Create the imputer for cleaning missing categorical variables
categorical_imputer_pipe2 = SimpleImputer(strategy='most_frequent')

# Create the discretizer for numerical variables to make them categorical
discretizer_pipe2 = KBinsDiscretizer(n_bins=20, strategy = "uniform", encode='ordinal')

# Creates the increasing ordinal encoder for categorical variables as required
ordinal_encoder_pipe2 = OrdinalEncoder(categories = [['A', 'B', 'C']])

# Create the selector of the most informative variables using chi2.
# The choice is made to use chi2 because there are now categorical features and categorical target.
selector_pipe2 = SelectKBest(chi2, k = 5)

# Make pipeline for numeric variables
pipe2_numerical = make_pipeline(numerical_imputer_pipe2, discretizer_pipe2)

# Make pipeline for categorical variables
pipe2_categorical = make_pipeline(categorical_imputer_pipe2, ordinal_encoder_pipe2)

# Create a column transformer to apply pipelines to specify variables in the dataset
ct_pipe2 = ColumnTransformer([
                              ('numerical_pipe2', pipe2_numerical, make_column_selector(dtype_exclude=['object', 'boolean', 'category'])),
                              ('categorical_pipe2', pipe2_categorical, make_column_selector(dtype_include=['object', 'boolean', 'category']))
                              ])

# Make the column transformer a pipeline
final_pipe2 = Pipeline([
                        ('transform_pipe2', ct_pipe2),
                        ('selector', selector_pipe2)
                        ])

"""##*Testing Pipeline 2*"""

# Applying Pipeline 2 on the dataset obtained from Pipeline 1
X_pipe2 = final_pipe2.fit_transform(X_pipe1,y)

# Make a dataframe
X_pipe2 = myt.to_dataframe(X_pipe2)

# Show the new dataset
X_pipe2

"""The Pipline 2 function returns the five most significant variables, as requested.

#**Pipeline 3**
"""

# Create the imputer for cleaning missing values
imputer_pipe3 = KNNImputer(n_neighbors=5, weights = 'distance')

# Create the standardiser to apply the PCA
standardiser_pipe3 = StandardScaler()

# Create the PCA applicator
pca_pipe3 = PCA(n_components = .8, random_state=0)

# Create a customised transformer.
# It identifies asymmetric variables according to a pre-set threshold and applies symmetry using the Yeo-Johnson method.
# See the symmetry analysis for the variables identified by PCA
transformer_pipe3 = myt.ConditionalSymmetrizer(threshold=0.25)

# Creates the scaler to return all variables to the same dimensionality
scaler_pipe3 = MinMaxScaler()

# # Create a column transformer to apply pipelines to numeric variables in the dataset
ct_pipe3 = ColumnTransformer([
                              ('pipe3_first_step', Pipeline([
                                                            ('cleaning_pipe3', imputer_pipe3),
                                                            ('standardizer_pipe3', standardiser_pipe3),
                                                            ('pca_pipe3', pca_pipe3),
                                                            ('transformer_asymmetric_pipe3', transformer_pipe3),
                                                            ('scaling_pipe3', scaler_pipe3)
                                                            ]), make_column_selector(dtype_exclude=['object', 'boolean', 'category'])),
                              ], remainder = 'passthrough')

# Make the column transformer a pipeline
final_pipe3 = Pipeline([
                        ('pipe3', ct_pipe3)
                        ])

"""##*Testing Pipeline 3*"""

# Applying Pipeline 3 on the dataset obtained from Pipeline 2
X_pipe3 = final_pipe3.fit_transform(X_pipe2)

# Make a dataframe
X_pipe3 = myt.to_dataframe(X_pipe3)

# Show the new dataset
X_pipe3

"""##*Inset: Symmetry analysis for variables identified by PCA*"""

# Create of a test pipeline to establish the skewness threshold
testing_pca_pipe3 = make_pipeline(imputer_pipe3, standardiser_pipe3, pca_pipe3)

# Applying test pipeline on the dataset obtained from Pipeline 2
X_testing_pipe3 = testing_pca_pipe3.fit_transform(X_pipe2)

# Make a dataframe
X_testing_pipe3 = myt.to_dataframe(X_testing_pipe3)

# Show the new dataset
X_testing_pipe3

myt.generate_symmetry_graphs(X_testing_pipe3, graph = 'hist', figsize = (24, 4), x_max_input = 0.35, y_max_input = 0.95, skew_threshold = 0.25)

"""A threshold of 0.25 has been identified as the point at which variable 0, which is less symmetrical from the graph, can be made to appear more symmetrical.

#**Pipeline finale**
"""

# Create a custom transformer to return an array in a dataframe
to_dataframe_transformer = FunctionTransformer(myt.to_dataframe, validate=True)

# Build a final pipeline with all the pipelines created
final_pipeline = Pipeline([
                          ('pipe1', final_pipe1),
                          ('to_dataframe1', to_dataframe_transformer),
                          ('pipe2', final_pipe2),
                          ('to_dataframe2', to_dataframe_transformer),
                          ('pipe3', final_pipe3)
                          ])

# Applies the final pipeline to the original dataset
X_final = final_pipeline.fit_transform(X,y)

# Make a dataframe
X_final = myt.to_dataframe(X_final)

# Show the new dataset
X_final

"""#**Exporting final pipeline**"""

dump(final_pipeline, 'pipeline.joblib')