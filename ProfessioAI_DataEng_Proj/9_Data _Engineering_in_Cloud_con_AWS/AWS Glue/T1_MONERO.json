{
	"jobConfig": {
		"name": "T1_MONERO",
		"description": "",
		"role": "arn:aws:iam::575108947083:role/Glue-role",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "T1_MONERO.py",
		"scriptLocation": "s3://aws-glue-assets-575108947083-eu-north-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-10-14T10:16:36.735Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-575108947083-eu-north-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-575108947083-eu-north-1/sparkHistoryLogs/",
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom pyspark.sql.window import Window\r\nfrom pyspark.sql.functions import regexp_replace, when, col, avg, last, to_date, round\r\n\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\r\nsc = SparkContext.getOrCreate()\r\nglueContext = GlueContext(sc)\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# Read price data from S3\r\ndf = glueContext.create_dynamic_frame.from_options(\r\n    connection_type=\"s3\",\r\n    connection_options={\"paths\": [\"s3://s3-sg-bucket-raw/MONERO/XMR_EUR Kraken Historical Data.csv\"]},\r\n    format=\"csv\",\r\n    format_options={\"withHeader\": True}\r\n)\r\n\r\n# Convert DynamicFrames to DataFrames\r\ndf = df.toDF()\r\ntrends_df = trends_df.toDF()\r\n\r\n# Drop unnecessary columns\r\ndf = df.drop('Open', 'High', 'Low', 'Vol.', 'Change %')\r\n\r\n# Convert 'Date' column to date type\r\ndf = df.withColumn('Date', to_date(df['Date'], 'MM/dd/yyyy'))\r\n\r\n# Convert 'Price' column to float type\r\ndf = df.withColumn('Price', regexp_replace(col('Price'), ',', '').cast('float'))\r\n\r\n# Define a 5-day window for calculating the average\r\nwindowSpec5 = Window.orderBy(\"Date\").rowsBetween(-5, -1)\r\n\r\n# Calculate the 5-day moving average for non-missing values\r\ndf = df.withColumn('Price_Avg_5', avg(when(col('Price') != -1, col('Price'))).over(windowSpec5))\r\n\r\n# Replace -1 with the calculated 5-day average\r\ndf = df.withColumn(\r\n    'Price',\r\n    when(df['Price'] == -1, df['Price_Avg_5']).otherwise(df['Price'])\r\n)\r\n\r\n# Round 'Price' values to two decimal places\r\ndf = df.withColumn('Price', round(df['Price'], 2))\r\n\r\n# Remove the auxiliary column if not needed\r\ndf = df.drop('Price_Avg_5')\r\n\r\n# Save the cleaned data to Parquet format on S3\r\ndf.write.mode('overwrite').parquet(\"s3://s3-sg-bucket-silver/MONERO\")\r\n\r\njob.commit()"
}